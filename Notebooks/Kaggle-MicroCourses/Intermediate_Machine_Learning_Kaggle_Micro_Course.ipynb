{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intermediate Machine Learning - Kaggle Micro Course.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNiVAB/t4GX/u+D7ymYzYrq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Svensone/Kaggle-Micro-Course/blob/master/Intermediate_Machine_Learning_Kaggle_Micro_Course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Du9-Kpf_dSZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV4zHrzidjeH",
        "colab_type": "text"
      },
      "source": [
        "# Intermediate Machine Learning\n",
        "\n",
        "Table of Contents\n",
        "1. Introduction\n",
        "2. Missing Values (Imputation or .dropna())\n",
        "3. Categorical Variables\n",
        "4. Pipelines\n",
        "5. Cross-Validation\n",
        "6. XGBoosting\n",
        "7. Data Leakage (target , train-test-contamination)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjIGbPVcv0Th",
        "colab_type": "text"
      },
      "source": [
        "## 2. Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFcD89CKqyXZ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4fr7jCaIs8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 2. Missing Values\n",
        "\n",
        "os.symlink()\n",
        ".select_dtypes(exclude=[\"object\"])\n",
        "\n",
        "X_train.drop([colums, ...], axis = 1, inplace = True)\n",
        "if df[col].isnull().any\n",
        ".fillna(ewma, ) # from thinkstats\n",
        "\n",
        "my_imputer = SimpleImputer()\n",
        "my_imputer.fit_transfrom(X_train)\n",
        "\n",
        "# thinkstats2 . mean moving average (windows with averge) \n",
        "# or expone. weighted moving average"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaxGPKdVbxM6",
        "colab_type": "text"
      },
      "source": [
        "### 3. Categorical Variables\n",
        "\n",
        "- ordinal (Label encoder) and nominal variables (One-Hot-Encoder)\n",
        "- cardinality of cat. variables (number of unique entries)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoEv5GnAcJfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select categorical columns\n",
        "\n",
        "cat_col = df.dtypes == 'object']\n",
        "no_cat = df.select_dtypes(exclude = \"cat_col\") # also 'include' possible\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NJkxEmKdhho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 3.Catecorical Variables\n",
        "\n",
        "1) drop column\n",
        "2) Label Encoding (Ordinal variables)\n",
        "3) One Hot Encoding (Nominal variables, not used if >15 diff. values not assume an order of the categories)\n",
        "\n",
        "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
        "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
        "                        X_train_full[cname].dtype == \"object\"]\n",
        "\n",
        "# Select numerical columns\n",
        "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
        "\n",
        "# Get list of categorical variables\n",
        "s = (X_train.dtypes == 'object')\n",
        "object_cols = list(s[s].index)\n",
        "\n",
        "# Option 1: Drop\n",
        "df = df.select_dtype(include= [np.number]) # keep only numerical columns\n",
        "# possible also\n",
        "select_dtype(exclude= ['object']) \n",
        "\n",
        "# Option 2: Labelencoder() from sklearn.preprocessing\n",
        "# LabelEncoder() needs same categories in train and valid data, otherwise error\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in object_cols:\n",
        "    label_X_train[col] = label_encoder(fit_transfrom(X_train[col]))\n",
        "    label_X_valid[col] = label_encoder(transform(X_valid[col]))\n",
        "\n",
        "# Option 3: ONe HOt Encoding\n",
        "# For large datasets with many rows, one-hot encoding can greatly \n",
        "# expand the size of the dataset. For this reason, we typically will \n",
        "# only one-hot encode columns with relatively low cardinality. \n",
        "# Then, high cardinality columns can \n",
        "# either be dropped from the dataset, or we can use label encoding\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Apply one-hot encoder to each column with categorical data\n",
        "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
        "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
        "\n",
        "# One-hot encoding removed index; put it back\n",
        "OH_cols_train.index = X_train.index\n",
        "OH_cols_valid.index = X_valid.index\n",
        "\n",
        "# Remove categorical columns (will replace with one-hot encoding)\n",
        "num_X_train = X_train.drop(object_cols, axis=1)\n",
        "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
        "\n",
        "# Add one-hot encoded columns to numerical features\n",
        "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
        "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfxKvqSKRsne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train[cols].nunique() < 10 # low and high cardinality"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuMi4ziBq7aj",
        "colab_type": "text"
      },
      "source": [
        "## 4. Pipeline\n",
        "\n",
        "my_pipeline = Pipeline(steps/transformers = [()])\n",
        "\n",
        "numerical_columns = SimpleImputer(strategy= 'media,mean, most_frequent')\n",
        "\n",
        "catergorical_columns = ColumnTransformer([])\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB-5DS0YsJMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Preprocessing for numerical data\n",
        "numerical_transformer = SimpleImputer(strategy='constant')\n",
        "\n",
        "# Preprocessing for categorical data\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Bundle preprocessing for numerical and categorical data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXphCmvtr8II",
        "colab_type": "text"
      },
      "source": [
        "## 5. Cross-Validation\n",
        "finding the best parameter / for best combination of hyperparameters use GridSearchCV\n",
        "\n",
        "- good on small dataset (couple minutes for training the model)\n",
        "- otherwise not, enough data for valid-set\n",
        "- cross_val_score(model, X, y, cv= , scoring = 'neg_mean_absolute_error')\n",
        "\n",
        "misc\n",
        ".range(from, to, steps)\n",
        "\n",
        "dictionary\n",
        "results= {}\n",
        "for x in range(50, 450, 50):\n",
        "  results[x] = get_score[x]\n",
        "\n",
        "min_results = min(results, key=results.get)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohNlIG61CmWH",
        "colab_type": "text"
      },
      "source": [
        "## 6. XGBoost\n",
        "\n",
        "\"best technology for standard tabular data (i.e. Dataframe data)\"\n",
        "\n",
        "\n",
        "from XGBoost import XGBRegressor\n",
        "model = XGBRegressor(n_estimators = Usually 500-1000 (with early stopping), learning_rate = smaller than 1e-2 good but slow training, n_jobs = 4 (parallel training, good on big datasets)\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          early_stopping_rounds = 5,\n",
        "          eval_set= [(x_valid, y_valid)],\n",
        "          verbose= False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc5YqJdSJYmN",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOw00ytHLhEd",
        "colab_type": "text"
      },
      "source": [
        "## 7. Data Leakage\n",
        "\n",
        "\n",
        "target leakage and train-test-contamination\n",
        "\n",
        "1. timinig / chronological (antibiotic intake and pneumonia - antibiotic intake after detection of pneumonia since target leakage and needs to be removed)\n",
        "\n",
        "2. train-test contamination\n",
        "data preprocessing (like fitting Imputer) before train-test-split\n",
        "\n",
        "remedies:\n",
        "\n",
        "- common sense in evaluating the columns of new data.\n",
        "- Pipelines to avoid train-test-contamination \n",
        "- caution-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3RjIG7VCk8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}